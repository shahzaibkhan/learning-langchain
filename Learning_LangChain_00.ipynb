{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kz3bfnPXKQdy",
        "iJySZcg0LEzy",
        "rkPRT_JSLQ4l",
        "us4YM7FdNMw_"
      ],
      "authorship_tag": "ABX9TyMerYI55zMtpdC4nb+MrFkn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahzaibkhan/learning-langchain/blob/main/Learning_LangChain_00.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Intro to LangChain**\n",
        "\n",
        "LangChain is a popular framework that allow users to quickly build apps and pipelines around Large Language Models. It can be used to for chatbots, Generative Question-Anwering (GQA), summarization, and much more.\n",
        "\n",
        "The core idea of the library is that we can \"chain\" together different components to create more advanced use-cases around LLMs. Chains may consist of multiple components from several modules:\n",
        "\n",
        "**Prompt templates:** Prompt *templates* are, well, templates for different types of prompts. Like \"chatbot\" style templates, ELI5 question-answering, etc\n",
        "\n",
        "**LLMs:** Large language models like GPT-3, BLOOM, etc\n",
        "\n",
        "**Agents:** Agents use LLMs to decide what actions should be taken, tools like web search or calculators can be used, and all packaged into logical loop of operations.\n",
        "\n",
        "**Memory:** Short-term memory, long-term memory."
      ],
      "metadata": {
        "id": "X-SoMYYLJ9eO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing LangChain**\n",
        "To install LangChain, you can use the following command:"
      ],
      "metadata": {
        "id": "kz3bfnPXKQdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CHJSb4-KfBl",
        "outputId": "ca972a75-1a35-44c1-bd33-1f941852459f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.0/90.0 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.6/149.6 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.1/49.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Using LLMs in LangChain**\n",
        "LangChain supports several LLM providers, like Hugging Face and OpenAI.\n",
        "\n",
        "Let's start our exploration of LangChain by learning how to use a few of these different LLM integrations.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iJySZcg0LEzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hugging Face\n",
        "We first need to install additional prerequisite libraries:\n"
      ],
      "metadata": {
        "id": "rkPRT_JSLQ4l"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_s4DlwNKPbBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU huggingface_hub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ylb1398TLwr0",
        "outputId": "4d7d0b9d-749a-4839-d578-54cc95574e5e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/236.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For Hugging Face models we need a Hugging Face Hub API token. We can find this by first getting an account at [HuggingFace.co](https://huggingface.co/) and clicking on our profile in the top-right corner > click *Settings* > click *Access Tokens* > click *New Token* > set *Role* to *write* > *Generate* > copy and paste the token below:"
      ],
      "metadata": {
        "id": "02-Nl6S4L05G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'"
      ],
      "metadata": {
        "id": "FPotZhpNMHZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, HuggingFaceHub, LLMChain\n",
        "\n",
        "# initialize HF LLM\n",
        "flan_t5 = HuggingFaceHub(\n",
        "    repo_id=\"google/flan-t5-xl\",\n",
        "    model_kwargs={\"temperature\":1e-10}\n",
        ")\n",
        "\n",
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=flan_t5\n",
        ")\n",
        "\n",
        "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "id": "hfaGZW8lMemw",
        "outputId": "0164ca91-282d-45d6-9ee7-0fed62ee2834"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-ccff11e25df3>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPromptTemplate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHuggingFaceHub\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# initialize HF LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m flan_t5 = HuggingFaceHub(\n\u001b[1;32m      5\u001b[0m     \u001b[0mrepo_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"google/flan-t5-xl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## OpenAI\n",
        "\n",
        "Start by installing additional prerequisites:"
      ],
      "metadata": {
        "id": "us4YM7FdNMw_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU openai"
      ],
      "metadata": {
        "id": "zQOGEN4dNPaw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use OpenAI's generative models. The process is similar, we need to\n",
        "give our API key which can be retrieved by signing up for an account on the\n",
        "[OpenAI website](https://openai.com/api/) (see top-right of page). We then pass the API key below:"
      ],
      "metadata": {
        "id": "ogQ26oa2NSY-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = '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'"
      ],
      "metadata": {
        "id": "ShSEYTXDNYYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we decide on which model we'd like to use, there are several options but we will go with text-davinci-003:"
      ],
      "metadata": {
        "id": "hD8Npv1_N4ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "\n",
        "davinci = OpenAI(model_name='text-davinci-003')"
      ],
      "metadata": {
        "id": "6VBtuEMJN9ut"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM davinci:"
      ],
      "metadata": {
        "id": "ZyiC5E8MOHix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate, LLMChain\n",
        "\n",
        "# build prompt template for simple question-answering\n",
        "template = \"\"\"Question: {question}\n",
        "\n",
        "Answer: \"\"\"\n",
        "prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
        "\n",
        "question = \"Which NFL team won the Super Bowl in the 2010 season?\"\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "print(llm_chain.run(question))"
      ],
      "metadata": {
        "id": "CaRqj8YlOHUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "If we'd like to ask multiple questions we can by passing a list of dictionary objects, where the dictionaries must contain the input variable set in our prompt template (\"question\") that is mapped to the question we'd like to ask."
      ],
      "metadata": {
        "id": "gdojd_dqqaRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    {'question': \"Which NFL team won the Super Bowl in the 2010 season?\"},\n",
        "    {'question': \"If I am 6 ft 4 inches, how tall am I in centimeters?\"},\n",
        "    {'question': \"Who was the 12th person on the moon?\"},\n",
        "    {'question': \"How many eyes does a blade of grass have?\"}\n",
        "]"
      ],
      "metadata": {
        "id": "e6GXT5F6qbxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llm_chain.generate(qs)"
      ],
      "metadata": {
        "id": "Vcvaj2tOqwho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that the below format doesn't feed the questions in iteratively but instead all in one chunk."
      ],
      "metadata": {
        "id": "By2QuWtMt5_d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "qs = [\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\",\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\",\n",
        "    \"Who was the 12th person on the moon?\",\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        "]\n",
        "print(llm_chain.run(qs))"
      ],
      "metadata": {
        "id": "Un2Crf4Tt7NN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can try to answer all question in one go, as mentioned, more powerful LLMs like `text-davinci-003` will be more likely to handle these more complex queries."
      ],
      "metadata": {
        "id": "1Dib_IcCuH9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "multi_template = \"\"\"Answer the following questions one at a time.\n",
        "\n",
        "Questions:\n",
        "{questions}\n",
        "\n",
        "Answers:\n",
        "\"\"\"\n",
        "long_prompt = PromptTemplate(\n",
        "    template=multi_template,\n",
        "    input_variables=[\"questions\"]\n",
        ")\n",
        "\n",
        "llm_chain = LLMChain(\n",
        "    prompt=long_prompt,\n",
        "    llm=davinci\n",
        ")\n",
        "\n",
        "qs_str = (\n",
        "    \"Which NFL team won the Super Bowl in the 2010 season?\\n\" +\n",
        "    \"If I am 6 ft 4 inches, how tall am I in centimeters?\\n\" +\n",
        "    \"Who was the 12th person on the moon?\" +\n",
        "    \"How many eyes does a blade of grass have?\"\n",
        ")\n",
        "\n",
        "print(llm_chain.run(qs_str))"
      ],
      "metadata": {
        "id": "rzT9Dt6quNtC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}