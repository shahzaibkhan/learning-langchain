{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNU9Ns4y2xb2Jz1vqYBYbmI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shahzaibkhan/learning-langchain/blob/main/Learning_LangChain_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Leanring Models with LangChain**\n",
        "\n",
        "A model is a program which is trained to complete a specific task.\n",
        "\n",
        "Note:\n",
        "- For current case, we be using Language Models.\n",
        "- We are not going to train a model\n",
        "- We will only be using trained models.\n",
        "\n",
        "These pre-trained models are trained on large amounts of data and require a lot of compute to run and thus are called Large Language Models (LLM).\n",
        "\n"
      ],
      "metadata": {
        "id": "Cepr9HBtctA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LLM (Large Language Model)**\n",
        "\n",
        "Now let's talk about LLM. LLM are trained to do language tasks like text generation. There are various LLM in the market but we are going to cover only OpenAI.\n",
        "\n",
        "So lets dive in directly learning how to use models with langchain.\n",
        "\n",
        "Let's install necessary libraries:\n"
      ],
      "metadata": {
        "id": "os2igIMidZrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain\n",
        "!pip install openai\n",
        "\n",
        "# The tiktoken package is a Byte Pair Encoding tokenizer. It is used with the OpenAI models and breaks down text into tokens.\n",
        "# This is used because the provided strings are sometimes a bit long for the specified OpenAI model.\n",
        "# So, it splits the text and encodes them into tokens. Now, let’s work on the main project.\n",
        "!pip install tiktoken"
      ],
      "metadata": {
        "id": "khRbqDjnd8VI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets set the OPEN API KEY\n",
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"]=\"YOUR_API_KEY”"
      ],
      "metadata": {
        "id": "oM_eUSMojKCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use a model here is how it is done:"
      ],
      "metadata": {
        "id": "shrsnJs2d-2c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "llm = OpenAI(temperature=1)\n",
        "print(llm(\"Write a positive quote about life\"))"
      ],
      "metadata": {
        "id": "G3qAKvfKd-s6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Estimating number of tokens**\n",
        "\n",
        "OpenAI models have a context length limiting the size of input data which can be sent to the model. Thus we need to make sure the input text is below that limit before sending to the model. We can do that token calculation using the code below:"
      ],
      "metadata": {
        "id": "3Kb_y0YoeXph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "llm.get_num_tokens(\"Write a positive quote about life\")"
      ],
      "metadata": {
        "id": "PhXEPB19ejIs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Streaming**\n",
        "\n",
        "Streaming is an important concept in LLM which allows you to display output on the go instead of waiting for the full output. Even in the ChatGPT interface you will see content streamed instead of waiting till entire output is generated\n",
        "\n",
        "Here is a code example for the same. We handle streaming in langchain using a callback handler:"
      ],
      "metadata": {
        "id": "c99mPHkTer8u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.llms import OpenAI\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "llm = OpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0)\n",
        "resp = llm(\"Write me a poem about life\")"
      ],
      "metadata": {
        "id": "FMfCPkRYe33m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Chat Models**\n",
        "\n",
        "The famous ChatGPT model GPT-3.5 comes under this. The main difference between the previous LLM models and the Chat Models are:\n",
        "\n",
        "- Chat Models are 10x cheaper for api calls\n",
        "- You can hold a conversation with a chat model like you can with a human which is not possible with the previous LLMs\n",
        "\n",
        "Since Chat Models can hold a conversation they take a list of chat messages as input instead of plain text like a LLM\n",
        "\n",
        "Now let's discuss how we can use these Chat Models.\n",
        "Let's do the necessary imports first:"
      ],
      "metadata": {
        "id": "2bBmoJyTe3bY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain import PromptTemplate, LLMChain\n",
        "from langchain.prompts.chat import (\n",
        "    ChatPromptTemplate,\n",
        "    SystemMessagePromptTemplate,\n",
        "    AIMessagePromptTemplate,\n",
        "    HumanMessagePromptTemplate,\n",
        ")\n",
        "from langchain.schema import (\n",
        "    AIMessage,\n",
        "    HumanMessage,\n",
        "    SystemMessage\n",
        ")"
      ],
      "metadata": {
        "id": "orjX-paMf23V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Instead of using a OpenAI class we will be using a ChatOpenAI class to create our chat LLM\n",
        "#Replace your OpenAI key here\n",
        "\n",
        "chat = ChatOpenAI(temperature=0, openai_api_key=\"openai-key\")\n"
      ],
      "metadata": {
        "id": "RyvzXPYtf8NI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input is a bunch of messages.**\n",
        "\n",
        "Messages are classified into 3 types\n",
        "\n",
        "1. **System Message** - This is an initial prompt sent to the model to control the behavior of the model.\n",
        "2. **Human Message** - Input message of the user\n",
        "3. **AI Message** - Message response given by ChatGPT\n",
        "\n",
        "ChatGPT needs a list of all these messages in the conversation to be able to understand the content and converse further.\n",
        "\n",
        "Now let's see an example where we define the system message and the message input of the user and pass to the chat model. The output generated will be an AI message.\n",
        "\n",
        "We are using a System Prompt to let the model do the task of paraphrasing. This technique of providing the model a prompt to make it perform a task is called Prompt Engineering."
      ],
      "metadata": {
        "id": "r0m99KAggGoc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    SystemMessage(content=\"Consider yourself as a expert in English Language who can help paraphrase sentences\"),\n",
        "    HumanMessage(content=\"I love programming.\")\n",
        "]\n",
        "chat(messages)"
      ],
      "metadata": {
        "id": "8LcPO17ugdEM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Templates in Chat Models**\n",
        "\n",
        "Earlier, we defined a system message with input variable task. This task can be dynamically change to do various tasks. For this example we will follow the task of paraphrasing.\n",
        "\n"
      ],
      "metadata": {
        "id": "RFzMCtWqgy2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "template=\"You are a helpful assistant that {task}.\"\n",
        "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
        "human_template=\"{text}\"\n",
        "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
        "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
        "\n",
        "chat(chat_prompt.format_prompt(task=\"paraphrases the sentence\", text=\"I love programming.\").to_messages())"
      ],
      "metadata": {
        "id": "1PNxahF1hDQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Chaining in Chat Models**\n",
        "Chaining multiple tasks for LLMs can be achieved with Chat Models as well. Here is an example:\n"
      ],
      "metadata": {
        "id": "Jvrpc7GqhUub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
        "chain.run(task=\"paraphrases the sentence\", text=\"I love programming.\")"
      ],
      "metadata": {
        "id": "kMHMvEShhf8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Streaming with Chat Models**\n",
        "As disucussed above how streaming can be useful in LLMs. Now let's see how we can do the same with Chat Models."
      ],
      "metadata": {
        "id": "PV8CizgMhn9v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace openai-key with your own key\n",
        "\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "chat = ChatOpenAI(streaming=True, callbacks=[StreamingStdOutCallbackHandler()], temperature=0, openai_api_key=\"openai-key\")\n",
        "resp = chat([HumanMessage(content=\"Write me a poem about life\")])"
      ],
      "metadata": {
        "id": "yWwnU7l5hzV3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding models**\n",
        "\n",
        "First we need to understand what is an embedding. An embedding is generally associated with a piece of text and it represents the properties of text.\n",
        "\n",
        "Just to give an example, let's consider the words good, best, bad. If we find the embeddings of these words we observe that embeddings of good and best are close while embedding of bad is far. The reason being embedding of a word has knowledge of the meaning of the word. Thus words with similar meanings have similar embeddings.\n",
        "\n",
        "Embeddings also have an **interesting** as can be seen below. Let's consider E(x) as Embedding of word x\n",
        "\n",
        "E(king) - E(male) + E(female) ~= E(queen)\n",
        "\n",
        "What this represents is if we subtract the embedding of word male from word king and add the embedding of word female it will be quite close to embedding of word queen. As humans we can understand this intuitively as removing male gender and adding female gender to king makes it a queen but now machines have the capability to understand such complex relations.\n",
        "\n",
        "Now that we have an idea of what is embeddings, the task of a embeddings model is to create these embeddings for the text input provided. A model which generates embedding which can show properties like the ones we discussed above and more is considered a good model.\n",
        "\n",
        "Once these embeddings are generated, we can use it to perform tasks like semantic search similar to how apps like Chatbase, PDF.ai, SiteGPT work. You can creating embeddings for all your documents or webpages and when user asks a query you can fetch the relevant pieces and send to the user\n",
        "\n",
        "Now let's discuss it with the help of an example:\n"
      ],
      "metadata": {
        "id": "y8OaPtL4iGaj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace openai-key with your own key\n",
        "\n",
        "from langchain.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(openai_api_key=\"openai-key\")\n",
        "strings = [\"This is for demonstration.\", \"This string is also for demonstration.\", \"This is another demo string.\", \"This one is last string.\"]\n",
        "result = model.embed_documents(strings)\n",
        "\n",
        "# To identify if sentences are similar, we can calculate the distance between these vectors.\n",
        "# If the distance is small, then the words are of similar meanings\n",
        "print(result)"
      ],
      "metadata": {
        "id": "hibXfIUZiwxz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}